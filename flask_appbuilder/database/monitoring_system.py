"""
Advanced Monitoring and Alerting System

Comprehensive monitoring solution for the graph analytics platform providing
real-time metrics, performance tracking, health checks, and intelligent alerting.
"""

import logging
import time
import threading
import json
import smtplib
import requests
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Union, Callable
from dataclasses import dataclass, asdict
from enum import Enum
from collections import deque, defaultdict
from concurrent.futures import ThreadPoolExecutor
from email.mime.text import MimeText
from email.mime.multipart import MimeMultipart

import psutil
import numpy as np
from pydantic import BaseModel, Field
from uuid_extensions import uuid7str

from ..utils.error_handling import WizardErrorHandler, WizardErrorType, WizardErrorSeverity

logger = logging.getLogger(__name__)


class AlertSeverity(Enum):
	"""Alert severity levels"""
	LOW = "low"
	MEDIUM = "medium"
	HIGH = "high"
	CRITICAL = "critical"


class MetricType(Enum):
	"""Types of metrics being monitored"""
	COUNTER = "counter"
	GAUGE = "gauge"
	HISTOGRAM = "histogram"
	TIMER = "timer"


class AlertStatus(Enum):
	"""Status of alerts"""
	ACTIVE = "active"
	ACKNOWLEDGED = "acknowledged"
	RESOLVED = "resolved"
	SUPPRESSED = "suppressed"


@dataclass
class Metric:
	"""Individual metric data point"""
	name: str
	value: Union[int, float]
	timestamp: datetime
	labels: Dict[str, str] = None
	metric_type: MetricType = MetricType.GAUGE
	
	def to_dict(self) -> Dict[str, Any]:
		return {
			"name": self.name,
			"value": self.value,
			"timestamp": self.timestamp.isoformat(),
			"labels": self.labels or {},
			"type": self.metric_type.value
		}


@dataclass
class Alert:
	"""Alert definition and current state"""
	alert_id: str
	name: str
	description: str
	severity: AlertSeverity
	condition: str
	threshold: Union[int, float]
	status: AlertStatus
	created_at: datetime
	last_triggered: Optional[datetime] = None
	acknowledgment_time: Optional[datetime] = None
	resolved_time: Optional[datetime] = None
	suppressed_until: Optional[datetime] = None
	trigger_count: int = 0
	metadata: Dict[str, Any] = None
	
	def to_dict(self) -> Dict[str, Any]:
		return {
			"alert_id": self.alert_id,
			"name": self.name,
			"description": self.description,
			"severity": self.severity.value,
			"condition": self.condition,
			"threshold": self.threshold,
			"status": self.status.value,
			"created_at": self.created_at.isoformat(),
			"last_triggered": self.last_triggered.isoformat() if self.last_triggered else None,
			"acknowledgment_time": self.acknowledgment_time.isoformat() if self.acknowledgment_time else None,
			"resolved_time": self.resolved_time.isoformat() if self.resolved_time else None,
			"suppressed_until": self.suppressed_until.isoformat() if self.suppressed_until else None,
			"trigger_count": self.trigger_count,
			"metadata": self.metadata or {}
		}


class MetricsCollector:
	"""Collects and stores system and application metrics"""
	
	def __init__(self, retention_period_hours: int = 24):
		self.metrics_buffer = defaultdict(lambda: deque(maxlen=1000))
		self.retention_period = timedelta(hours=retention_period_hours)
		self.collection_interval = 5  # seconds
		self.running = False
		self.collection_thread = None
		self.custom_collectors = {}
		
		# System metrics to collect
		self.system_metrics = {
			"cpu_percent": self._collect_cpu_percent,
			"memory_percent": self._collect_memory_percent,
			"disk_usage_percent": self._collect_disk_usage,
			"network_io": self._collect_network_io,
			"database_connections": self._collect_db_connections,
			"active_sessions": self._collect_active_sessions,
			"query_response_time": self._collect_query_response_time,
			"error_rate": self._collect_error_rate
		}
		
		logger.info("Metrics collector initialized")
	
	def start_collection(self):
		"""Start automatic metrics collection"""
		if self.running:
			return
			
		self.running = True
		self.collection_thread = threading.Thread(target=self._collection_loop, daemon=True)
		self.collection_thread.start()
		logger.info("Started metrics collection")
	
	def stop_collection(self):
		"""Stop metrics collection"""
		self.running = False
		if self.collection_thread:
			self.collection_thread.join(timeout=10)
		logger.info("Stopped metrics collection")
	
	def _collection_loop(self):
		"""Main collection loop"""
		while self.running:
			try:
				self._collect_all_metrics()
				time.sleep(self.collection_interval)
			except Exception as e:
				logger.error(f"Error in metrics collection loop: {e}")
				time.sleep(self.collection_interval)
	
	def _collect_all_metrics(self):
		"""Collect all configured metrics"""
		current_time = datetime.now()
		
		# Collect system metrics
		for metric_name, collector_func in self.system_metrics.items():
			try:
				value = collector_func()
				if value is not None:
					metric = Metric(
						name=metric_name,
						value=value,
						timestamp=current_time,
						metric_type=MetricType.GAUGE
					)
					self.record_metric(metric)
			except Exception as e:
				logger.error(f"Failed to collect metric {metric_name}: {e}")
		
		# Collect custom metrics
		for metric_name, collector_func in self.custom_collectors.items():
			try:
				value = collector_func()
				if value is not None:
					metric = Metric(
						name=metric_name,
						value=value,
						timestamp=current_time,
						metric_type=MetricType.GAUGE
					)
					self.record_metric(metric)
			except Exception as e:
				logger.error(f"Failed to collect custom metric {metric_name}: {e}")
		
		# Clean up old metrics
		self._cleanup_old_metrics()
	
	def _collect_cpu_percent(self) -> float:
		"""Collect CPU usage percentage"""
		return psutil.cpu_percent(interval=1)
	
	def _collect_memory_percent(self) -> float:
		"""Collect memory usage percentage"""
		return psutil.virtual_memory().percent
	
	def _collect_disk_usage(self) -> float:
		"""Collect disk usage percentage"""
		return psutil.disk_usage('/').percent
	
	def _collect_network_io(self) -> Dict[str, int]:
		"""Collect network I/O statistics"""
		net_io = psutil.net_io_counters()
		return {
			"bytes_sent": net_io.bytes_sent,
			"bytes_recv": net_io.bytes_recv
		}
	
	def _collect_db_connections(self) -> int:
		"""Collect active database connections"""
		# This would integrate with your database connection pool
		try:
			# Mock implementation - replace with actual connection pool metrics
			return len(threading.enumerate())  # Placeholder
		except:
			return 0
	
	def _collect_active_sessions(self) -> int:
		"""Collect number of active user sessions"""
		# This would integrate with your session management
		try:
			# Mock implementation - replace with actual session counting
			return 5  # Placeholder
		except:
			return 0
	
	def _collect_query_response_time(self) -> float:
		"""Collect average query response time"""
		# This would integrate with your query execution tracking
		try:
			# Mock implementation - replace with actual response time tracking
			return 0.125  # Placeholder
		except:
			return 0.0
	
	def _collect_error_rate(self) -> float:
		"""Collect error rate percentage"""
		# This would integrate with your error tracking
		try:
			# Mock implementation - replace with actual error rate calculation
			return 0.5  # Placeholder
		except:
			return 0.0
	
	def record_metric(self, metric: Metric):
		"""Record a metric data point"""
		self.metrics_buffer[metric.name].append(metric)
	
	def add_custom_collector(self, metric_name: str, collector_func: Callable[[], Any]):
		"""Add a custom metric collector"""
		self.custom_collectors[metric_name] = collector_func
		logger.info(f"Added custom collector for metric: {metric_name}")
	
	def get_metrics(self, metric_name: str, time_range_minutes: int = 60) -> List[Metric]:
		"""Get metrics for a specific time range"""
		cutoff_time = datetime.now() - timedelta(minutes=time_range_minutes)
		metrics = []
		
		for metric in self.metrics_buffer.get(metric_name, []):
			if metric.timestamp >= cutoff_time:
				metrics.append(metric)
		
		return sorted(metrics, key=lambda m: m.timestamp)
	
	def get_metric_summary(self, metric_name: str, time_range_minutes: int = 60) -> Dict[str, float]:
		"""Get statistical summary of a metric"""
		metrics = self.get_metrics(metric_name, time_range_minutes)
		
		if not metrics:
			return {"count": 0}
		
		values = [m.value for m in metrics if isinstance(m.value, (int, float))]
		
		if not values:
			return {"count": 0}
		
		return {
			"count": len(values),
			"min": min(values),
			"max": max(values),
			"mean": np.mean(values),
			"median": np.median(values),
			"std_dev": np.std(values),
			"percentile_95": np.percentile(values, 95),
			"percentile_99": np.percentile(values, 99)
		}
	
	def _cleanup_old_metrics(self):
		"""Clean up metrics older than retention period"""
		cutoff_time = datetime.now() - self.retention_period
		
		for metric_name, metrics in self.metrics_buffer.items():
			# Remove old metrics
			while metrics and metrics[0].timestamp < cutoff_time:
				metrics.popleft()


class AlertManager:
	"""Manages alert definitions, evaluation, and notifications"""
	
	def __init__(self, metrics_collector: MetricsCollector):
		self.metrics_collector = metrics_collector
		self.alerts = {}
		self.alert_rules = {}
		self.notification_channels = {}
		self.evaluation_interval = 30  # seconds
		self.running = False
		self.evaluation_thread = None
		
		# Built-in alert rules
		self._setup_default_alerts()
		
		logger.info("Alert manager initialized")
	
	def _setup_default_alerts(self):
		"""Setup default system alerts"""
		default_rules = [
			{
				"name": "High CPU Usage",
				"condition": "cpu_percent > threshold",
				"threshold": 80.0,
				"severity": AlertSeverity.HIGH,
				"description": "CPU usage is above 80%"
			},
			{
				"name": "High Memory Usage",
				"condition": "memory_percent > threshold",
				"threshold": 85.0,
				"severity": AlertSeverity.HIGH,
				"description": "Memory usage is above 85%"
			},
			{
				"name": "Disk Space Low",
				"condition": "disk_usage_percent > threshold",
				"threshold": 90.0,
				"severity": AlertSeverity.CRITICAL,
				"description": "Disk usage is above 90%"
			},
			{
				"name": "High Error Rate",
				"condition": "error_rate > threshold",
				"threshold": 5.0,
				"severity": AlertSeverity.MEDIUM,
				"description": "Error rate is above 5%"
			},
			{
				"name": "Slow Query Response",
				"condition": "query_response_time > threshold",
				"threshold": 2.0,
				"severity": AlertSeverity.MEDIUM,
				"description": "Average query response time is above 2 seconds"
			}
		]
		
		for rule in default_rules:
			self.create_alert_rule(
				name=rule["name"],
				condition=rule["condition"],
				threshold=rule["threshold"],
				severity=rule["severity"],
				description=rule["description"]
			)
	
	def create_alert_rule(self, name: str, condition: str, threshold: Union[int, float], 
						  severity: AlertSeverity, description: str, 
						  metadata: Dict[str, Any] = None) -> str:
		"""Create a new alert rule"""
		alert_id = uuid7str()
		
		alert = Alert(
			alert_id=alert_id,
			name=name,
			description=description,
			severity=severity,
			condition=condition,
			threshold=threshold,
			status=AlertStatus.ACTIVE,
			created_at=datetime.now(),
			metadata=metadata
		)
		
		self.alerts[alert_id] = alert
		self.alert_rules[alert_id] = {
			"condition_func": self._parse_condition(condition),
			"metric_name": self._extract_metric_name(condition)
		}
		
		logger.info(f"Created alert rule: {name} ({alert_id})")
		return alert_id
	
	def _parse_condition(self, condition: str) -> Callable[[float, float], bool]:
		"""Parse condition string into evaluatable function"""
		if ">" in condition:
			return lambda value, threshold: value > threshold
		elif "<" in condition:
			return lambda value, threshold: value < threshold
		elif ">=" in condition:
			return lambda value, threshold: value >= threshold
		elif "<=" in condition:
			return lambda value, threshold: value <= threshold
		elif "==" in condition:
			return lambda value, threshold: value == threshold
		else:
			return lambda value, threshold: False
	
	def _extract_metric_name(self, condition: str) -> str:
		"""Extract metric name from condition string"""
		# Simple extraction - could be more sophisticated
		parts = condition.split()
		return parts[0] if parts else ""
	
	def start_evaluation(self):
		"""Start alert evaluation loop"""
		if self.running:
			return
			
		self.running = True
		self.evaluation_thread = threading.Thread(target=self._evaluation_loop, daemon=True)
		self.evaluation_thread.start()
		logger.info("Started alert evaluation")
	
	def stop_evaluation(self):
		"""Stop alert evaluation"""
		self.running = False
		if self.evaluation_thread:
			self.evaluation_thread.join(timeout=10)
		logger.info("Stopped alert evaluation")
	
	def _evaluation_loop(self):
		"""Main alert evaluation loop"""
		while self.running:
			try:
				self._evaluate_all_alerts()
				time.sleep(self.evaluation_interval)
			except Exception as e:
				logger.error(f"Error in alert evaluation loop: {e}")
				time.sleep(self.evaluation_interval)
	
	def _evaluate_all_alerts(self):
		"""Evaluate all active alert rules"""
		for alert_id, alert in self.alerts.items():
			if alert.status in [AlertStatus.ACTIVE, AlertStatus.ACKNOWLEDGED]:
				try:
					self._evaluate_alert(alert_id)
				except Exception as e:
					logger.error(f"Failed to evaluate alert {alert.name}: {e}")
	
	def _evaluate_alert(self, alert_id: str):
		"""Evaluate a single alert rule"""
		alert = self.alerts[alert_id]
		rule = self.alert_rules[alert_id]
		
		# Get recent metric values
		metrics = self.metrics_collector.get_metrics(rule["metric_name"], time_range_minutes=5)
		
		if not metrics:
			return
		
		# Use most recent value
		current_value = metrics[-1].value
		
		# Evaluate condition
		condition_met = rule["condition_func"](current_value, alert.threshold)
		
		if condition_met and alert.status != AlertStatus.ACTIVE:
			# Alert triggered
			alert.status = AlertStatus.ACTIVE
			alert.last_triggered = datetime.now()
			alert.trigger_count += 1
			
			self._send_alert_notification(alert, current_value)
			logger.warning(f"Alert triggered: {alert.name} (current value: {current_value})")
		
		elif not condition_met and alert.status == AlertStatus.ACTIVE:
			# Alert resolved
			alert.status = AlertStatus.RESOLVED
			alert.resolved_time = datetime.now()
			
			self._send_resolution_notification(alert, current_value)
			logger.info(f"Alert resolved: {alert.name} (current value: {current_value})")
	
	def acknowledge_alert(self, alert_id: str, user_id: str = None):
		"""Acknowledge an alert"""
		if alert_id in self.alerts:
			alert = self.alerts[alert_id]
			alert.status = AlertStatus.ACKNOWLEDGED
			alert.acknowledgment_time = datetime.now()
			
			if alert.metadata is None:
				alert.metadata = {}
			alert.metadata["acknowledged_by"] = user_id
			
			logger.info(f"Alert acknowledged: {alert.name} by {user_id}")
			return True
		return False
	
	def suppress_alert(self, alert_id: str, duration_minutes: int):
		"""Suppress an alert for specified duration"""
		if alert_id in self.alerts:
			alert = self.alerts[alert_id]
			alert.status = AlertStatus.SUPPRESSED
			alert.suppressed_until = datetime.now() + timedelta(minutes=duration_minutes)
			
			logger.info(f"Alert suppressed: {alert.name} for {duration_minutes} minutes")
			return True
		return False
	
	def get_active_alerts(self) -> List[Alert]:
		"""Get all active alerts"""
		return [
			alert for alert in self.alerts.values()
			if alert.status in [AlertStatus.ACTIVE, AlertStatus.ACKNOWLEDGED]
		]
	
	def get_alert_history(self, hours: int = 24) -> List[Alert]:
		"""Get alert history for specified time period"""
		cutoff_time = datetime.now() - timedelta(hours=hours)
		return [
			alert for alert in self.alerts.values()
			if alert.created_at >= cutoff_time
		]
	
	def add_notification_channel(self, channel_name: str, channel_config: Dict[str, Any]):
		"""Add notification channel (email, webhook, etc.)"""
		self.notification_channels[channel_name] = channel_config
		logger.info(f"Added notification channel: {channel_name}")
	
	def _send_alert_notification(self, alert: Alert, current_value: float):
		"""Send alert notification through configured channels"""
		message = f"ALERT: {alert.name}\n"
		message += f"Description: {alert.description}\n"
		message += f"Severity: {alert.severity.value.upper()}\n"
		message += f"Current Value: {current_value}\n"
		message += f"Threshold: {alert.threshold}\n"
		message += f"Time: {alert.last_triggered}\n"
		
		for channel_name, config in self.notification_channels.items():
			try:
				self._send_notification(channel_name, config, "Alert Triggered", message)
			except Exception as e:
				logger.error(f"Failed to send alert notification via {channel_name}: {e}")
	
	def _send_resolution_notification(self, alert: Alert, current_value: float):
		"""Send alert resolution notification"""
		message = f"RESOLVED: {alert.name}\n"
		message += f"Current Value: {current_value}\n"
		message += f"Resolved at: {alert.resolved_time}\n"
		
		for channel_name, config in self.notification_channels.items():
			try:
				self._send_notification(channel_name, config, "Alert Resolved", message)
			except Exception as e:
				logger.error(f"Failed to send resolution notification via {channel_name}: {e}")
	
	def _send_notification(self, channel_name: str, config: Dict[str, Any], subject: str, message: str):
		"""Send notification through specific channel"""
		if config.get("type") == "email":
			self._send_email_notification(config, subject, message)
		elif config.get("type") == "webhook":
			self._send_webhook_notification(config, subject, message)
		elif config.get("type") == "slack":
			self._send_slack_notification(config, subject, message)
	
	def _send_email_notification(self, config: Dict[str, Any], subject: str, message: str):
		"""Send email notification"""
		try:
			msg = MimeMultipart()
			msg['From'] = config["from_email"]
			msg['To'] = config["to_email"]
			msg['Subject'] = f"[Graph Analytics] {subject}"
			
			msg.attach(MimeText(message, 'plain'))
			
			server = smtplib.SMTP(config["smtp_server"], config.get("smtp_port", 587))
			server.starttls()
			server.login(config["username"], config["password"])
			server.send_message(msg)
			server.quit()
			
		except Exception as e:
			logger.error(f"Failed to send email notification: {e}")
	
	def _send_webhook_notification(self, config: Dict[str, Any], subject: str, message: str):
		"""Send webhook notification"""
		try:
			payload = {
				"subject": subject,
				"message": message,
				"timestamp": datetime.now().isoformat(),
				"source": "graph_analytics_platform"
			}
			
			response = requests.post(
				config["webhook_url"],
				json=payload,
				headers=config.get("headers", {}),
				timeout=30
			)
			response.raise_for_status()
			
		except Exception as e:
			logger.error(f"Failed to send webhook notification: {e}")
	
	def _send_slack_notification(self, config: Dict[str, Any], subject: str, message: str):
		"""Send Slack notification"""
		try:
			payload = {
				"text": f"*{subject}*\n```{message}```",
				"channel": config.get("channel", "#alerts"),
				"username": "Graph Analytics Bot",
				"icon_emoji": ":warning:"
			}
			
			response = requests.post(
				config["webhook_url"],
				json=payload,
				timeout=30
			)
			response.raise_for_status()
			
		except Exception as e:
			logger.error(f"Failed to send Slack notification: {e}")


class HealthChecker:
	"""System health monitoring and diagnostics"""
	
	def __init__(self, metrics_collector: MetricsCollector):
		self.metrics_collector = metrics_collector
		self.health_checks = {}
		self.last_check_results = {}
		
		# Register default health checks
		self._register_default_checks()
		
		logger.info("Health checker initialized")
	
	def _register_default_checks(self):
		"""Register default system health checks"""
		self.register_health_check("database_connectivity", self._check_database_connectivity)
		self.register_health_check("memory_usage", self._check_memory_usage)
		self.register_health_check("disk_space", self._check_disk_space)
		self.register_health_check("response_time", self._check_response_time)
		self.register_health_check("error_rate", self._check_error_rate)
	
	def register_health_check(self, check_name: str, check_func: Callable[[], Dict[str, Any]]):
		"""Register a custom health check"""
		self.health_checks[check_name] = check_func
		logger.info(f"Registered health check: {check_name}")
	
	def run_health_check(self, check_name: str) -> Dict[str, Any]:
		"""Run a specific health check"""
		if check_name not in self.health_checks:
			return {"status": "error", "message": f"Health check '{check_name}' not found"}
		
		try:
			start_time = time.time()
			result = self.health_checks[check_name]()
			execution_time = time.time() - start_time
			
			result["execution_time"] = execution_time
			result["timestamp"] = datetime.now().isoformat()
			result["check_name"] = check_name
			
			self.last_check_results[check_name] = result
			return result
			
		except Exception as e:
			error_result = {
				"status": "error",
				"message": str(e),
				"timestamp": datetime.now().isoformat(),
				"check_name": check_name
			}
			self.last_check_results[check_name] = error_result
			return error_result
	
	def run_all_health_checks(self) -> Dict[str, Dict[str, Any]]:
		"""Run all registered health checks"""
		results = {}
		
		with ThreadPoolExecutor(max_workers=5) as executor:
			future_to_check = {
				executor.submit(self.run_health_check, check_name): check_name
				for check_name in self.health_checks.keys()
			}
			
			for future in future_to_check:
				check_name = future_to_check[future]
				try:
					results[check_name] = future.result(timeout=30)
				except Exception as e:
					results[check_name] = {
						"status": "error",
						"message": f"Health check timed out or failed: {str(e)}",
						"timestamp": datetime.now().isoformat()
					}
		
		return results
	
	def get_system_health_summary(self) -> Dict[str, Any]:
		"""Get overall system health summary"""
		all_results = self.run_all_health_checks()
		
		total_checks = len(all_results)
		healthy_checks = sum(1 for result in all_results.values() if result.get("status") == "healthy")
		warning_checks = sum(1 for result in all_results.values() if result.get("status") == "warning")
		error_checks = sum(1 for result in all_results.values() if result.get("status") == "error")
		
		overall_status = "healthy"
		if error_checks > 0:
			overall_status = "unhealthy"
		elif warning_checks > 0:
			overall_status = "degraded"
		
		return {
			"overall_status": overall_status,
			"total_checks": total_checks,
			"healthy_checks": healthy_checks,
			"warning_checks": warning_checks,
			"error_checks": error_checks,
			"health_score": (healthy_checks / total_checks * 100) if total_checks > 0 else 0,
			"timestamp": datetime.now().isoformat(),
			"details": all_results
		}
	
	def _check_database_connectivity(self) -> Dict[str, Any]:
		"""Check database connectivity"""
		try:
			# Mock database check - replace with actual database ping
			time.sleep(0.01)  # Simulate database query
			
			return {
				"status": "healthy",
				"message": "Database connection is healthy",
				"response_time_ms": 10
			}
		except Exception as e:
			return {
				"status": "error",
				"message": f"Database connectivity failed: {str(e)}"
			}
	
	def _check_memory_usage(self) -> Dict[str, Any]:
		"""Check memory usage levels"""
		try:
			memory_percent = psutil.virtual_memory().percent
			
			if memory_percent > 90:
				status = "error"
				message = f"Critical memory usage: {memory_percent:.1f}%"
			elif memory_percent > 80:
				status = "warning"
				message = f"High memory usage: {memory_percent:.1f}%"
			else:
				status = "healthy"
				message = f"Memory usage normal: {memory_percent:.1f}%"
			
			return {
				"status": status,
				"message": message,
				"memory_percent": memory_percent
			}
		except Exception as e:
			return {
				"status": "error",
				"message": f"Memory check failed: {str(e)}"
			}
	
	def _check_disk_space(self) -> Dict[str, Any]:
		"""Check disk space availability"""
		try:
			disk_usage = psutil.disk_usage('/').percent
			
			if disk_usage > 95:
				status = "error"
				message = f"Critical disk usage: {disk_usage:.1f}%"
			elif disk_usage > 85:
				status = "warning"
				message = f"High disk usage: {disk_usage:.1f}%"
			else:
				status = "healthy"
				message = f"Disk usage normal: {disk_usage:.1f}%"
			
			return {
				"status": status,
				"message": message,
				"disk_usage_percent": disk_usage
			}
		except Exception as e:
			return {
				"status": "error",
				"message": f"Disk check failed: {str(e)}"
			}
	
	def _check_response_time(self) -> Dict[str, Any]:
		"""Check average response time"""
		try:
			# Get recent response time metrics
			summary = self.metrics_collector.get_metric_summary("query_response_time", time_range_minutes=5)
			
			if summary["count"] == 0:
				return {
					"status": "warning",
					"message": "No recent response time data available"
				}
			
			avg_response_time = summary["mean"]
			
			if avg_response_time > 5.0:
				status = "error"
				message = f"Critical response time: {avg_response_time:.2f}s"
			elif avg_response_time > 2.0:
				status = "warning"
				message = f"Slow response time: {avg_response_time:.2f}s"
			else:
				status = "healthy"
				message = f"Response time normal: {avg_response_time:.2f}s"
			
			return {
				"status": status,
				"message": message,
				"avg_response_time": avg_response_time,
				"p95_response_time": summary.get("percentile_95", 0)
			}
		except Exception as e:
			return {
				"status": "error",
				"message": f"Response time check failed: {str(e)}"
			}
	
	def _check_error_rate(self) -> Dict[str, Any]:
		"""Check system error rate"""
		try:
			# Get recent error rate metrics
			summary = self.metrics_collector.get_metric_summary("error_rate", time_range_minutes=10)
			
			if summary["count"] == 0:
				return {
					"status": "warning",
					"message": "No recent error rate data available"
				}
			
			avg_error_rate = summary["mean"]
			
			if avg_error_rate > 10.0:
				status = "error"
				message = f"Critical error rate: {avg_error_rate:.1f}%"
			elif avg_error_rate > 5.0:
				status = "warning"
				message = f"High error rate: {avg_error_rate:.1f}%"
			else:
				status = "healthy"
				message = f"Error rate normal: {avg_error_rate:.1f}%"
			
			return {
				"status": status,
				"message": message,
				"avg_error_rate": avg_error_rate,
				"max_error_rate": summary.get("max", 0)
			}
		except Exception as e:
			return {
				"status": "error",
				"message": f"Error rate check failed: {str(e)}"
			}


class MonitoringSystem:
	"""Main monitoring system orchestrator"""
	
	def __init__(self):
		self.metrics_collector = MetricsCollector()
		self.alert_manager = AlertManager(self.metrics_collector)
		self.health_checker = HealthChecker(self.metrics_collector)
		self.error_handler = WizardErrorHandler()
		
		logger.info("Monitoring system initialized")
	
	def start(self):
		"""Start all monitoring components"""
		try:
			self.metrics_collector.start_collection()
			self.alert_manager.start_evaluation()
			logger.info("Monitoring system started")
		except Exception as e:
			self.error_handler.handle_error(
				e, WizardErrorType.SYSTEM_ERROR, WizardErrorSeverity.HIGH
			)
			raise
	
	def stop(self):
		"""Stop all monitoring components"""
		try:
			self.metrics_collector.stop_collection()
			self.alert_manager.stop_evaluation()
			logger.info("Monitoring system stopped")
		except Exception as e:
			logger.error(f"Error stopping monitoring system: {e}")
	
	def get_dashboard_data(self) -> Dict[str, Any]:
		"""Get comprehensive dashboard data"""
		try:
			# Get system health
			health_summary = self.health_checker.get_system_health_summary()
			
			# Get active alerts
			active_alerts = [alert.to_dict() for alert in self.alert_manager.get_active_alerts()]
			
			# Get key metrics
			key_metrics = {}
			metric_names = ["cpu_percent", "memory_percent", "disk_usage_percent", "query_response_time"]
			
			for metric_name in metric_names:
				summary = self.metrics_collector.get_metric_summary(metric_name, time_range_minutes=60)
				if summary["count"] > 0:
					key_metrics[metric_name] = {
						"current": summary["mean"],
						"trend": "stable",  # Could implement trend analysis
						"p95": summary.get("percentile_95", 0),
						"min": summary["min"],
						"max": summary["max"]
					}
			
			# Get recent metric history
			metric_history = {}
			for metric_name in metric_names:
				metrics = self.metrics_collector.get_metrics(metric_name, time_range_minutes=60)
				metric_history[metric_name] = [
					{"timestamp": m.timestamp.isoformat(), "value": m.value}
					for m in metrics[-20:]  # Last 20 data points
				]
			
			return {
				"health_summary": health_summary,
				"active_alerts": active_alerts,
				"key_metrics": key_metrics,
				"metric_history": metric_history,
				"timestamp": datetime.now().isoformat()
			}
			
		except Exception as e:
			self.error_handler.handle_error(
				e, WizardErrorType.DATA_PROCESSING_ERROR, WizardErrorSeverity.MEDIUM
			)
			return {
				"error": "Failed to generate dashboard data",
				"timestamp": datetime.now().isoformat()
			}


# Global monitoring system instance
_monitoring_system = None


def get_monitoring_system() -> MonitoringSystem:
	"""Get the global monitoring system instance"""
	global _monitoring_system
	if _monitoring_system is None:
		_monitoring_system = MonitoringSystem()
	return _monitoring_system


def initialize_monitoring():
	"""Initialize and start the monitoring system"""
	monitoring = get_monitoring_system()
	monitoring.start()
	return monitoring


def shutdown_monitoring():
	"""Shutdown the monitoring system"""
	global _monitoring_system
	if _monitoring_system:
		_monitoring_system.stop()
		_monitoring_system = None