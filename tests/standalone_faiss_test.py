#!/usr/bin/env python3
"""
Standalone FAISS Integration Test

Tests FAISS components in isolation without Flask-AppBuilder dependencies
to validate the core FAISS integration functionality.
"""

import os
import sys
import tempfile
import shutil
import time
from typing import List, Dict, Any, Optional
from enum import Enum
from dataclasses import dataclass

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

print("üß™ Testing FAISS Integration Components (Standalone)")
print("=" * 60)

# Test FAISS availability first
try:
    import faiss
    import numpy as np
    print("‚úÖ FAISS library available")
    FAISS_AVAILABLE = True
except ImportError as e:
    print(f"‚ùå FAISS library not available: {e}")
    FAISS_AVAILABLE = False
    sys.exit(1)

# Define minimal required types to avoid Flask-AppBuilder imports
class DocumentType(Enum):
    """Document types for testing."""
    TEXT = "text"
    CODE = "code"
    HTML = "html"
    MARKDOWN = "markdown"

@dataclass
class DocumentChunk:
    """Document chunk for testing."""
    content: str
    metadata: Dict[str, Any]
    chunk_index: int = 0
    document_id: str = ""
    document_type: DocumentType = DocumentType.TEXT
    embeddings: Optional[List[float]] = None

# Mock AI Model for testing
class MockEmbeddingModel:
    """Mock embedding model for testing."""

    def __init__(self, embedding_dim: int = 768):
        self.embedding_dim = embedding_dim

    async def get_embedding(self, text: str) -> List[float]:
        """Generate deterministic mock embeddings."""
        import hashlib

        # Create reproducible embedding from text hash
        hash_bytes = hashlib.sha256(text.encode()).digest()
        embedding = []

        # Convert hash bytes to float values
        for i in range(0, min(len(hash_bytes), self.embedding_dim * 4), 4):
            if i + 4 <= len(hash_bytes):
                # Convert 4 bytes to float
                int_val = int.from_bytes(hash_bytes[i:i+4], byteorder='little', signed=True)
                float_val = int_val / (2**31)  # Normalize to [-1, 1]
                embedding.append(float_val)

        # Pad or truncate to desired dimension
        while len(embedding) < self.embedding_dim:
            embedding.append(0.0)
        embedding = embedding[:self.embedding_dim]

        # Normalize to unit vector
        norm = sum(x * x for x in embedding) ** 0.5
        if norm > 0:
            embedding = [x / norm for x in embedding]

        return embedding

# Test basic FAISS operations
def test_faiss_basic_operations():
    """Test basic FAISS operations."""
    print("\nüîç Testing FAISS Basic Operations...")

    try:
        # Create FAISS index
        dim = 768
        index = faiss.IndexFlatL2(dim)

        # Generate test vectors
        np.random.seed(42)  # Reproducible results
        test_vectors = np.random.random((50, dim)).astype('float32')
        faiss.normalize_L2(test_vectors)  # Normalize for cosine similarity

        # Add vectors to index
        index.add(test_vectors)
        print(f"‚úÖ Added {index.ntotal} vectors to Flat index")

        # Test search
        query_vectors = test_vectors[:3]  # Use first 3 as queries
        distances, indices = index.search(query_vectors, k=5)

        print(f"‚úÖ Search completed - {len(distances)} queries processed")
        print(f"   Sample distances: {distances[0][:3]}")
        print(f"   Sample indices: {indices[0][:3]}")

        # Test different index types
        index_types = ["Flat", "IVF100,Flat"]
        for idx_type in index_types:
            try:
                test_index = faiss.index_factory(dim, idx_type)
                if hasattr(test_index, 'train') and not test_index.is_trained:
                    test_index.train(test_vectors)
                test_index.add(test_vectors[:10])  # Smaller dataset for complex indices
                print(f"‚úÖ {idx_type} index created and populated")
            except Exception as e:
                print(f"‚ö†Ô∏è  {idx_type} index failed: {e}")

        return True

    except Exception as e:
        print(f"‚ùå FAISS basic operations failed: {e}")
        return False

# Test FAISS Vector Store components (standalone)
def test_faiss_vector_store_standalone():
    """Test FAISS vector store components without database dependencies."""
    print("\nüîç Testing FAISS Vector Store (Standalone)...")

    try:
        # Import just the FAISS-specific components
        sys.path.append('/Users/nyimbiodero/src/pjs/fab-ext')

        # Test IndexConfig
        class IndexConfig:
            def __init__(self, index_type="Flat", embedding_dim=768, use_gpu=False):
                self.index_type = index_type
                self.embedding_dim = embedding_dim
                self.use_gpu = use_gpu

            def get_index_string(self, collection_size: int = 0) -> str:
                if self.index_type == "auto":
                    if collection_size < 1000:
                        return "Flat"
                    elif collection_size < 10000:
                        return "IVF100,Flat"
                    else:
                        return "IVF1000,PQ64"
                return self.index_type

        config = IndexConfig("Flat", 768, False)
        print(f"‚úÖ IndexConfig created: {config.index_type}")

        # Test index creation
        dim = 768
        index_string = config.get_index_string(500)
        index = faiss.index_factory(dim, index_string)
        print(f"‚úÖ FAISS index created with string: '{index_string}'")

        # Test auto-selection logic
        auto_config = IndexConfig("auto", 768, False)
        small_index = auto_config.get_index_string(100)
        medium_index = auto_config.get_index_string(5000)
        large_index = auto_config.get_index_string(50000)

        print(f"‚úÖ Auto index selection:")
        print(f"   Small dataset (100): {small_index}")
        print(f"   Medium dataset (5000): {medium_index}")
        print(f"   Large dataset (50000): {large_index}")

        return True

    except Exception as e:
        print(f"‚ùå FAISS vector store test failed: {e}")
        return False

# Test document processing components
async def test_document_processing():
    """Test document processing without framework dependencies."""
    print("\nüîç Testing Document Processing...")

    try:
        # Simple chunking logic
        def simple_chunk_text(text: str, chunk_size: int = 500, overlap: int = 100) -> List[str]:
            chunks = []
            start = 0
            while start < len(text):
                end = min(start + chunk_size, len(text))
                chunk = text[start:end]
                if chunk.strip():
                    chunks.append(chunk.strip())
                start = end - overlap
                if start >= len(text):
                    break
            return chunks

        # Test with different document types
        test_documents = {
            "text": "This is a simple text document. It contains multiple sentences for testing chunking algorithms. The document should be split into appropriate chunks based on the configured chunk size and overlap parameters.",
            "html": "<h1>HTML Document</h1><p>This is an HTML document with <strong>formatted</strong> content. It includes <em>various</em> HTML tags that should be processed appropriately.</p>",
            "code": """
def example_function():
    '''This is a code example.'''
    result = []
    for i in range(10):
        if i % 2 == 0:
            result.append(i * 2)
    return result

class ExampleClass:
    def __init__(self):
        self.value = 0
        """,
            "markdown": "# Markdown Document\n\nThis is a **markdown** document with *formatting*.\n\n## Section 2\n\nIt includes lists:\n- Item 1\n- Item 2\n\nAnd code blocks:\n```python\nprint('hello world')\n```"
        }

        # Test chunking for each document type
        for doc_type, content in test_documents.items():
            chunks = simple_chunk_text(content, chunk_size=200, overlap=50)
            print(f"‚úÖ {doc_type.upper()} document: {len(chunks)} chunks created")

            # Create DocumentChunk objects
            doc_chunks = []
            for i, chunk_content in enumerate(chunks):
                doc_chunk = DocumentChunk(
                    content=chunk_content,
                    metadata={
                        "source_type": doc_type,
                        "chunk_index": i,
                        "total_chunks": len(chunks)
                    },
                    chunk_index=i,
                    document_id=f"test_{doc_type}",
                    document_type=getattr(DocumentType, doc_type.upper(), DocumentType.TEXT)
                )
                doc_chunks.append(doc_chunk)

            print(f"   Sample chunk length: {len(doc_chunks[0].content)} chars")

        print("‚úÖ Document processing test completed")
        return True

    except Exception as e:
        print(f"‚ùå Document processing test failed: {e}")
        return False

# Test end-to-end RAG pipeline (simplified)
async def test_rag_pipeline():
    """Test simplified RAG pipeline."""
    print("\nüîç Testing RAG Pipeline (Simplified)...")

    try:
        # Setup
        embedding_model = MockEmbeddingModel(768)
        dim = 768
        index = faiss.IndexFlatL2(dim)

        # Test documents
        documents = [
            "Flask-AppBuilder is a rapid application development framework.",
            "FAISS provides efficient similarity search for dense vectors.",
            "Vector databases enable semantic search capabilities.",
            "Machine learning embeddings capture semantic relationships.",
            "Python is a popular programming language for AI applications."
        ]

        print(f"Processing {len(documents)} test documents...")

        # Generate embeddings and add to index
        doc_embeddings = []
        for i, doc in enumerate(documents):
            embedding = await embedding_model.get_embedding(doc)
            doc_embeddings.append(embedding)

            # Convert to numpy array and add to FAISS
            embedding_array = np.array([embedding], dtype=np.float32)
            index.add(embedding_array)

        print(f"‚úÖ Added {index.ntotal} documents to FAISS index")

        # Test queries
        test_queries = [
            "web development framework",
            "vector search library",
            "semantic search technology"
        ]

        search_results = []
        for query in test_queries:
            # Generate query embedding
            query_embedding = await embedding_model.get_embedding(query)
            query_array = np.array([query_embedding], dtype=np.float32)

            # Search FAISS index
            distances, indices = index.search(query_array, k=3)

            # Collect results
            results = []
            for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):
                if idx != -1:  # Valid result
                    results.append({
                        "document": documents[idx],
                        "similarity_score": float(1 / (1 + distance)),  # Convert distance to similarity
                        "distance": float(distance)
                    })

            search_results.append({
                "query": query,
                "results": results
            })

            print(f"‚úÖ Query: '{query}' - Found {len(results)} results")
            if results:
                print(f"   Best match: '{results[0]['document'][:50]}...'")
                print(f"   Similarity: {results[0]['similarity_score']:.3f}")

        print("‚úÖ RAG pipeline test completed successfully")
        return True

    except Exception as e:
        print(f"‚ùå RAG pipeline test failed: {e}")
        return False

# Performance benchmark
async def test_performance_benchmark():
    """Test FAISS performance with larger dataset."""
    print("\nüîç Testing Performance Benchmark...")

    try:
        dim = 768
        dataset_size = 1000

        # Generate larger test dataset
        np.random.seed(42)
        vectors = np.random.random((dataset_size, dim)).astype('float32')
        faiss.normalize_L2(vectors)

        # Test different index types
        index_configs = [
            ("Flat", "Flat"),
            ("IVF", "IVF100,Flat"),
        ]

        benchmark_results = {}

        for name, index_string in index_configs:
            try:
                # Create and populate index
                index = faiss.index_factory(dim, index_string)

                # Training for IVF indices
                if hasattr(index, 'is_trained') and not index.is_trained:
                    train_start = time.time()
                    index.train(vectors)
                    train_time = time.time() - train_start
                else:
                    train_time = 0

                # Add vectors
                add_start = time.time()
                index.add(vectors)
                add_time = time.time() - add_start

                # Search benchmark
                query_vectors = vectors[:10]  # Use first 10 as queries
                search_start = time.time()
                distances, indices = index.search(query_vectors, k=5)
                search_time = time.time() - search_start

                benchmark_results[name] = {
                    "train_time": train_time,
                    "add_time": add_time,
                    "search_time": search_time,
                    "vectors_per_sec_add": dataset_size / add_time if add_time > 0 else float('inf'),
                    "queries_per_sec": len(query_vectors) / search_time if search_time > 0 else float('inf')
                }

                print(f"‚úÖ {name} index benchmark:")
                print(f"   Add time: {add_time:.3f}s ({benchmark_results[name]['vectors_per_sec_add']:.1f} vec/s)")
                print(f"   Search time: {search_time:.3f}s ({benchmark_results[name]['queries_per_sec']:.1f} q/s)")
                if train_time > 0:
                    print(f"   Train time: {train_time:.3f}s")

            except Exception as e:
                print(f"‚ö†Ô∏è  {name} index benchmark failed: {e}")

        print("‚úÖ Performance benchmark completed")
        return True

    except Exception as e:
        print(f"‚ùå Performance benchmark failed: {e}")
        return False

# Main test execution
async def main():
    """Run all standalone FAISS integration tests."""
    print("Starting standalone FAISS integration tests...\n")

    tests = [
        ("FAISS Basic Operations", test_faiss_basic_operations),
        ("FAISS Vector Store", test_faiss_vector_store_standalone),
        ("Document Processing", test_document_processing),
        ("RAG Pipeline", test_rag_pipeline),
        ("Performance Benchmark", test_performance_benchmark)
    ]

    results = []
    for test_name, test_func in tests:
        try:
            if test_name in ["Document Processing", "RAG Pipeline", "Performance Benchmark"]:
                result = await test_func()
            else:
                result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"‚ùå {test_name} failed with exception: {e}")
            results.append((test_name, False))

    # Print summary
    print("\n" + "="*60)
    print("üìã STANDALONE FAISS TEST SUMMARY")
    print("="*60)

    passed = 0
    total = len(results)

    for test_name, result in results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{status:<8} {test_name}")
        if result:
            passed += 1

    print(f"\nüìä Results: {passed}/{total} tests passed")

    if passed == total:
        print("üéâ All standalone FAISS integration tests passed!")
        print("üí° The FAISS integration core functionality is working correctly.")
    elif passed > 0:
        print("‚ö†Ô∏è  Some tests failed, but core FAISS functionality is available.")
    else:
        print("üö® All tests failed. FAISS integration has issues.")

    print("\nNote: This validates core FAISS functionality.")
    print("Full Flask-AppBuilder integration requires resolving circular import issues.")

if __name__ == "__main__":
    import asyncio
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nüõë Tests interrupted by user")
    except Exception as e:
        print(f"\nüí• Test suite failed: {e}")
        import traceback
        traceback.print_exc()